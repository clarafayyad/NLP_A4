{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import nltk, string, random, numpy, os\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "os.makedirs('./results', exist_ok=True)\n",
    "nltk.download('brown')\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "numpy.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In the warm-up of this assignment, we will see how Neural Networks (NN) handle natural language data.   \n",
    "The warm-up focuses on a simple Multilayer Perceptron (MLP), also known as a fully connected Neural Network. The data we'll use is the first 5000 unique words of the Brown corpus.\n",
    "\n",
    "### Dataset\n",
    "To train the model, we will have to represent the input words to the model in some way. Since models solely work with numbers, the words will have to be converted into numerical form.  \n",
    "For this assignment, we will focus on predicting individual words from the dataset given the input of the model. The input will be the target word split up into individual letters. To represent these individual letters we will give the model a vector of 26 positions (26 letters in the English alphabet). Initially, this vector is filled with zeros and for every occuring letter in the word we change the value to 1 in that position. For instance, in the word `apple', we have 1 a, 1 e, 1 l and 2 p. The vector will then represent the word as:  \n",
    "```[1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,2,0,0,0,0,0,0,0,0,0,0]```  \n",
    "You will have to implement this algorithm together with loading the Brown dataset and taking the first 5000 unique words. Implement a way to store the indexes of the unique words as a dictionary where the word is the key and the index is the value as well as the target list (which will be just the indexes of the words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaDataset(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        # Load the NLTK Brown corpus and store the first 5000 unique words of the corpus in self.data\n",
    "        self.data = None\n",
    "        self.word_to_idx = None # Convert the unique words to an index dictionary {word: index}\n",
    "        self.targets = None # Make these indexes the target values\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # Torch requires the implementation of the length function to calculate the number of instances in the dataset. Find a way to implement this\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        # Apply processing to turn the word (stored in x) into a numeric vector of 26 numbers, counting the occurences of the letters.\n",
    "        # Example: apple would become [1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,2,0,0,0,0,0,0,0,0,0,0], counting 2 occurences for the letter p and zero for letters that do not occur.\n",
    "\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        return x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron\n",
    "In the following section, you will implement a MLP. The goal is to implement this MLP with 1 input layer, 2 hidden layers, and 1 output layer.\n",
    "With PyTorch, the linear layer is most suitable for this. When you create a linear layer, you define the input and output size of the layer, effectively creating two linear neuron layers. This is useful to know since we only need to create 3 linear layer classes to have the 4 layers we want.\n",
    "The hidden size is stored as a list where the first value will be 256 and the second value will be 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: List[int], output_size: int) -> None:\n",
    "        super().__init__()\n",
    "        # Implement the neural network layers, the activation function is already defined\n",
    "        self.input_layer = None\n",
    "        self.hidden_layer = None\n",
    "        self.output_layer = None\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # In the forward pass the model will calculate the gradients as well as the probabilities of the result occuring given its input.\n",
    "        # Implement the missing layers\n",
    "\n",
    "        x = None # Implement input layer\n",
    "        x = self.activation(x)\n",
    "        x = None # Implement hidden layer\n",
    "        x = self.activation(x)\n",
    "        x = None # Implement the output layer\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the hyperparameters\n",
    "These are the hyperparameters used for the model, they define the layout of the model as well as the performance:\n",
    "- batch_size, defines the number of instances the model sees at one time.\n",
    "- learning_rate, defines the change rate of the gradient descent.\n",
    "- input_size, the number of input neurons for the model, the number of letters in the alphabet\n",
    "- hidden_size, the number of neurons in the hidden layer\n",
    "- output_size, the number of neurons in the output layer, for us this is the number of unqiue words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "learning_rate = 1e-5\n",
    "input_size = 26\n",
    "hidden_size = [256, 512]\n",
    "output_size = 5000\n",
    "device = 'cpu' # If you have an m1 macbook use: 'mbp', if you have an NVIDIA GPU use: 'cuda:0' else leave as is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset and the dataloader\n",
    "dataset = AlphaDataset()\n",
    "\n",
    "# For the final evaluation of the model we will use 20% of the data for testing. Testing is only ever done after hyperparameter tuning.\n",
    "# Split sizes (80% train, 20% test)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# To evaluate our model we want to take 10% of the dataset for validation, this is similar to the testset, rather this data we can use during hyperparameter tuning.\n",
    "# The validation and test data is never trained on and is unseen data for the model, making it closer to a production setting.\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # Shuffling ensures the model does not overfit on ordering of the data.\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) # This data does not need to be shuffled\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)  #T his data does not need to be shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preperation for training\n",
    "Here we load the model into memory, apply it to the selected device and define the optimizer. The optimizer guides the model to the best possible state it can be in through Gradient descent.\n",
    "Lastly, the loss function is defined, this defines how well the model performs, based on this number the model knows how it should change its weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(input_size, hidden_size, output_size)\n",
    "model.to(device) # Tell de model which accelerator to use (Macbook GPU, NVIDIA GPU or CPU)\n",
    "\n",
    "# In Neural Networks optimizers handle the efficient training through gradient descent, we will use Adam\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# The loss function defines how well the model is performing, if the loss is low the model is rewarded, if it is high the model is punished.\n",
    "# Since we are dealing with a classification task we will use Cross Entropy\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "In the next block the training block is already defined. This is a standard way to train the model for 50 epochs (50 times it will see the dataset). Each time it does one epoch we also go over the validationset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "for epoch in range(50): # Train for 50 epochs\n",
    "    model.train() # Enforce model training\n",
    "    for step, (x, y) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        output = model(x)\n",
    "\n",
    "        loss = loss_fn(output, y)\n",
    "\n",
    "        training_losses.append(loss.item())\n",
    "        print(loss.item(), end='\\r')\n",
    "\n",
    "        loss.backward() # Calculate gradients\n",
    "\n",
    "        optimizer.step() # Reward the model\n",
    "        optimizer.zero_grad() # Clean the gradients\n",
    "    print('Training_loss:', loss.item())\n",
    "\n",
    "    model.eval() # After every training epoch we want to see the model's performance on the validation data\n",
    "    with torch.no_grad(): # In validation we dont need gradients so we tell torch to not calculate them\n",
    "        total_val_loss = 0\n",
    "        for step, (x, y) in enumerate(val_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            output = model(x)\n",
    "\n",
    "            loss = loss_fn(output, y)\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "        validation_losses.append(total_val_loss / len(val_dataset))\n",
    "        print('Validation_loss:', validation_losses[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing\n",
    "Below we can print the training statistics, the training loss should be going down while the validation loss should be going up. What does it mean that the validation loss increases?\n",
    "\n",
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.plot(training_losses)\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Training Loss')\n",
    "plt.savefig('./results/training_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(validation_losses)\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.savefig('./results/validation_loss.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing\n",
    "Belowe we want to analyze how the model functions based on the test data. What stands out from these results? Was the result correct? What is the main difference between the words?\n",
    "\n",
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_words(dataset, n, model, ds, verbose = True):\n",
    "    if n == -1:\n",
    "        n = len(dataset)\n",
    "    x = [dataset.__getitem__(i)[0] for i in range(n)]\n",
    "    y = [dataset.__getitem__(i)[1] for i in range(n)]\n",
    "    target_words = [{idx: word for word, idx in ds.word_to_idx.items()}[_.item()] for _ in y]\n",
    "    out = [nn.functional.softmax(model(wrd)).argmax() for wrd in x]\n",
    "    predicted_words = [{idx: word for word, idx in ds.word_to_idx.items()}[_.item()] for _ in out]\n",
    "    width = max(len(word) for word in target_words) + 5\n",
    "    if verbose:\n",
    "        print('\\n'.join([f'target: {t.ljust(width)} predicted: {p}' for t, p in zip(target_words, predicted_words)]))\n",
    "    return target_words, predicted_words\n",
    "\n",
    "_,_ = get_n_words(test_dataset, 10, model, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are some words incorrectly predicted?\n",
    "\n",
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code + Written\n",
    "Research the Jaccard similarity metric for calculating the difference between the predicted word and the target word. Reference your sources and implement this metric in your code below. You can use the get_n_words function with n=-1 to get all the predicted and target words. Compare this method to similarity as measured with one of the word vector methods from A2, in writing and/or code.\n",
    "\n",
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "midigpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
